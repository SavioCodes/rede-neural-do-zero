# ğŸ§  Rede Neural do Zero

Uma implementaÃ§Ã£o completa de uma rede neural artificial desenvolvida do zero em Python, sem uso de frameworks como TensorFlow ou PyTorch. Projeto educacional para entender os fundamentos do deep learning.

## ğŸ“‹ Ãndice

- [Sobre o Projeto](#sobre-o-projeto)
- [Teoria e Fundamentos](#teoria-e-fundamentos)
- [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
- [Uso](#uso)
- [Exemplos](#exemplos)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Funcionalidades](#funcionalidades)
- [Contribuindo](#contribuindo)
- [LicenÃ§a](#licenÃ§a)
- [CrÃ©ditos](#crÃ©ditos)

## ğŸ¯ Sobre o Projeto

Este projeto implementa uma rede neural artificial completamente do zero usando apenas NumPy e bibliotecas bÃ¡sicas do Python. O objetivo Ã© fornecer uma compreensÃ£o clara e detalhada de como funcionam as redes neurais internamente, incluindo:

- Forward propagation (propagaÃ§Ã£o direta)
- Backpropagation (retropropagaÃ§Ã£o)
- FunÃ§Ãµes de ativaÃ§Ã£o
- OtimizaÃ§Ã£o de pesos e biases
- Treinamento e validaÃ§Ã£o

## ğŸ“š Teoria e Fundamentos

### O que Ã© uma Rede Neural?

Uma rede neural artificial Ã© um modelo computacional inspirado no funcionamento do cÃ©rebro humano. Ela Ã© composta por neurÃ´nios artificiais (nÃ³s) organizados em camadas:

1. **Camada de Entrada**: Recebe os dados de input
2. **Camadas Ocultas**: Processam as informaÃ§Ãµes
3. **Camada de SaÃ­da**: Produz o resultado final

### Forward Propagation

O processo onde os dados fluem da entrada para a saÃ­da:

```
z = WÂ·x + b
a = f(z)
```

Onde:
- `z` Ã© a soma ponderada
- `W` sÃ£o os pesos
- `x` sÃ£o as entradas
- `b` Ã© o bias
- `f` Ã© a funÃ§Ã£o de ativaÃ§Ã£o
- `a` Ã© a saÃ­da ativada

### Backpropagation

Algoritmo usado para treinar a rede, calculando gradientes e atualizando pesos:

```
âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚a Â· âˆ‚a/âˆ‚z Â· âˆ‚z/âˆ‚W
```

### FunÃ§Ãµes de AtivaÃ§Ã£o Implementadas

1. **Sigmoid**: `Ïƒ(x) = 1/(1 + e^(-x))`
2. **ReLU**: `f(x) = max(0, x)`
3. **Tanh**: `f(x) = (e^x - e^(-x))/(e^x + e^(-x))`

## ğŸš€ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.7+
- pip

### InstalaÃ§Ã£o das DependÃªncias

```bash
# Clone o repositÃ³rio
git clone https://github.com/SavioCodes/rede-neural-do-zero.git
cd rede-neural-do-zero

# Instale as dependÃªncias
pip install -r requirements.txt
```

## ğŸ’» Uso

### Uso BÃ¡sico

```python
from src.rede_neural import RedeNeural
import numpy as np

# Criar uma rede neural com 2 entradas, 4 neurÃ´nios ocultos e 1 saÃ­da
rede = RedeNeural([2, 4, 1])

# Dados de exemplo (XOR)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# Treinar a rede
rede.treinar(X, y, epochs=1000, taxa_aprendizado=0.1)

# Fazer prediÃ§Ãµes
predicoes = rede.prever(X)
print("PrediÃ§Ãµes:", predicoes)
```

### Executar Exemplo Completo

```bash
python examples/exemplo.py
```

## ğŸ“Š Exemplos

### Problema XOR

O exemplo clÃ¡ssico do XOR demonstra a capacidade da rede de aprender padrÃµes nÃ£o-lineares:

```
Entrada: [0, 0] â†’ SaÃ­da Esperada: 0
Entrada: [0, 1] â†’ SaÃ­da Esperada: 1
Entrada: [1, 0] â†’ SaÃ­da Esperada: 1
Entrada: [1, 1] â†’ SaÃ­da Esperada: 0
```

**Resultados Esperados:**
```
Ã‰poca 0: Erro = 0.2500
Ã‰poca 100: Erro = 0.0847
Ã‰poca 500: Erro = 0.0234
Ã‰poca 1000: Erro = 0.0089

PrediÃ§Ãµes Finais:
[0, 0] â†’ 0.05 (â‰ˆ 0)
[0, 1] â†’ 0.94 (â‰ˆ 1)
[1, 0] â†’ 0.96 (â‰ˆ 1)
[1, 1] â†’ 0.07 (â‰ˆ 0)

AcurÃ¡cia: 97.5%
```

### ClassificaÃ§Ã£o com Dataset Personalizado

```python
# Exemplo com dados de classificaÃ§Ã£o binÃ¡ria
from src.utils import gerar_dataset_classificacao

X, y = gerar_dataset_classificacao(n_samples=1000, n_features=2)
rede = RedeNeural([2, 8, 4, 1], ativacao='relu')
rede.treinar(X, y, epochs=2000, taxa_aprendizado=0.01)
```

## ğŸ“ Estrutura do Projeto

```
rede-neural-do-zero/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rede_neural.py      # ImplementaÃ§Ã£o principal da rede neural
â”‚   â”œâ”€â”€ funcoes_ativacao.py # FunÃ§Ãµes de ativaÃ§Ã£o e suas derivadas
â”‚   â””â”€â”€ utils.py            # UtilitÃ¡rios para dados e visualizaÃ§Ã£o
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ xor_dataset.csv     # Dataset XOR
â”‚   â””â”€â”€ mnist_sample.csv    # Amostra do MNIST
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ exemplo.py          # Exemplo principal de uso
â”‚   â”œâ”€â”€ xor_exemplo.py      # Exemplo especÃ­fico do XOR
â”‚   â””â”€â”€ classificacao.py    # Exemplo de classificaÃ§Ã£o
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ teoria.md           # ExplicaÃ§Ã£o teÃ³rica detalhada
â”‚   â”œâ”€â”€ algoritmos.md       # Detalhes dos algoritmos
â”‚   â””â”€â”€ diagramas/          # Diagramas e visualizaÃ§Ãµes
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_rede_neural.py # Testes unitÃ¡rios
â”‚   â””â”€â”€ test_funcoes.py     # Testes das funÃ§Ãµes
â”œâ”€â”€ requirements.txt        # DependÃªncias do projeto
â”œâ”€â”€ LICENSE                 # LicenÃ§a MIT
â””â”€â”€ README.md              # Este arquivo
```

## âœ¨ Funcionalidades

### Implementado âœ…

- [x] Rede neural com mÃºltiplas camadas totalmente conectadas
- [x] Forward propagation completa
- [x] Backpropagation com gradiente descendente
- [x] FunÃ§Ãµes de ativaÃ§Ã£o: Sigmoid, ReLU, Tanh
- [x] InicializaÃ§Ã£o inteligente de pesos (Xavier/He)
- [x] MÃ©tricas de avaliaÃ§Ã£o (erro, acurÃ¡cia)
- [x] VisualizaÃ§Ã£o de treinamento
- [x] Exemplo XOR funcional
- [x] Suporte a datasets personalizados
- [x] Testes unitÃ¡rios
- [x] DocumentaÃ§Ã£o completa

### PrÃ³ximas Funcionalidades ğŸš§

- [ ] RegularizaÃ§Ã£o (L1, L2)
- [ ] Diferentes otimizadores (Adam, RMSprop)
- [ ] Batch normalization
- [ ] Dropout
- [ ] ValidaÃ§Ã£o cruzada
- [ ] Suporte a MNIST completo

## ğŸ§ª Testes

Execute os testes para verificar se tudo estÃ¡ funcionando:

```bash
python -m pytest tests/ -v
```

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Para contribuir:

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“– ReferÃªncias

- [Neural Networks and Deep Learning - Michael Nielsen](http://neuralnetworksanddeeplearning.com/)
- [Deep Learning - Ian Goodfellow](https://www.deeplearningbook.org/)
- [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [Backpropagation Algorithm](https://en.wikipedia.org/wiki/Backpropagation)

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ‘¨â€ğŸ’» CrÃ©ditos

Desenvolvido por [SÃ¡vio](https://github.com/SavioCodes)

---

â­ Se este projeto te ajudou, considere dar uma estrela no GitHub!

# üî¨ Detalhes dos Algoritmos

Este documento fornece uma vis√£o detalhada dos algoritmos implementados na rede neural.

## üßÆ Algoritmo de Forward Propagation

### Pseudoc√≥digo
```
fun√ß√£o forward_propagation(X):
    ativa√ß√µes = [X]  // Primeira ativa√ß√£o √© a entrada
    z_values = []
    
    para cada camada l de 1 at√© L:
        // Calcular soma ponderada
        z = ativa√ß√µes[l-1] √ó W[l] + b[l]
        z_values.adicionar(z)
        
        // Aplicar fun√ß√£o de ativa√ß√£o
        a = fun√ß√£o_ativa√ß√£o(z)
        ativa√ß√µes.adicionar(a)
    
    retornar ativa√ß√µes, z_values
```

### Implementa√ß√£o Python Simplificada
```python
def forward_propagation(self, X):
    ativacoes = [X]
    z_values = []
    
    for i in range(self.num_camadas - 1):
        z = np.dot(ativacoes[i], self.pesos[i]) + self.biases[i]
        z_values.append(z)
        a = self.funcao_ativacao(z)
        ativacoes.append(a)
    
    return ativacoes, z_values
```

### Complexidade
- **Temporal**: O(n √ó m √ó h), onde n = amostras, m = features, h = neur√¥nios
- **Espacial**: O(n √ó h) para armazenar ativa√ß√µes

## ‚¨ÖÔ∏è Algoritmo de Backpropagation

### Pseudoc√≥digo Detalhado
```
fun√ß√£o backpropagation(X, y, ativa√ß√µes, z_values):
    m = n√∫mero_de_amostras
    gradientes_W = []
    gradientes_b = []
    
    // Erro da √∫ltima camada
    delta = (ativa√ß√µes[-1] - y)
    
    // Backpropagation das camadas (de tr√°s para frente)
    para l de L-1 at√© 0:
        // Gradientes da camada atual
        dW = (1/m) √ó ativa√ß√µes[l]·µÄ √ó delta
        db = (1/m) √ó soma(delta, axis=0)
        
        gradientes_W.inserir_no_in√≠cio(dW)
        gradientes_b.inserir_no_in√≠cio(db)
        
        // Delta para camada anterior (se n√£o for a primeira)
        se l > 0:
            delta = (delta √ó W[l]·µÄ) ‚äô derivada_ativa√ß√£o(z_values[l-1])
    
    retornar gradientes_W, gradientes_b
```

### Deriva√ß√£o Matem√°tica

#### Regra da Cadeia
Para a camada de sa√≠da L:
```
‚àÇC/‚àÇW‚ÅΩ·¥∏‚Åæ = ‚àÇC/‚àÇa‚ÅΩ·¥∏‚Åæ √ó ‚àÇa‚ÅΩ·¥∏‚Åæ/‚àÇz‚ÅΩ·¥∏‚Åæ √ó ‚àÇz‚ÅΩ·¥∏‚Åæ/‚àÇW‚ÅΩ·¥∏‚Åæ
```

Onde:
- `‚àÇC/‚àÇa‚ÅΩ·¥∏‚Åæ = a‚ÅΩ·¥∏‚Åæ - y` (para MSE)
- `‚àÇa‚ÅΩ·¥∏‚Åæ/‚àÇz‚ÅΩ·¥∏‚Åæ = f'(z‚ÅΩ·¥∏‚Åæ)` (derivada da ativa√ß√£o)
- `‚àÇz‚ÅΩ·¥∏‚Åæ/‚àÇW‚ÅΩ·¥∏‚Åæ = a‚ÅΩ·¥∏‚Åª¬π‚Åæ` (entrada da camada)

#### Para Camadas Ocultas
```
‚àÇC/‚àÇW‚ÅΩÀ°‚Åæ = Œ¥‚ÅΩÀ°‚Åæ √ó (a‚ÅΩÀ°‚Åª¬π‚Åæ)·µÄ
```

Onde `Œ¥‚ÅΩÀ°‚Åæ` √© propagado de:
```
Œ¥‚ÅΩÀ°‚Åæ = (W‚ÅΩÀ°‚Å∫¬π‚Åæ)·µÄ √ó Œ¥‚ÅΩÀ°‚Å∫¬π‚Åæ ‚äô f'(z‚ÅΩÀ°‚Åæ)
```

### Complexidade
- **Temporal**: O(n √ó m √ó h) - mesma que forward prop
- **Espacial**: O(h¬≤) para armazenar gradientes

## üéØ Algoritmo de Treinamento

### Gradiente Descendente Completo
```
fun√ß√£o treinar(X, y, epochs, learning_rate):
    inicializar_pesos_aleatoriamente()
    
    para epoch de 1 at√© epochs:
        // Forward pass
        ativa√ß√µes, z_values = forward_propagation(X)
        
        // Calcular erro
        erro = calcular_erro(y, ativa√ß√µes[-1])
        
        // Backward pass
        grad_W, grad_b = backpropagation(X, y, ativa√ß√µes, z_values)
        
        // Atualizar par√¢metros
        para cada camada l:
            W[l] = W[l] - learning_rate √ó grad_W[l]
            b[l] = b[l] - learning_rate √ó grad_b[l]
        
        // Logging opcional
        se epoch % log_interval == 0:
            imprimir(f"√âpoca {epoch}: Erro = {erro}")
```

### Varia√ß√µes do Gradiente Descendente

#### 1. Batch Gradient Descent
- Usa todo o dataset para cada update
- Converg√™ncia est√°vel, mas lenta
- Implementa√ß√£o padr√£o neste projeto

#### 2. Stochastic Gradient Descent (SGD)
```python
def sgd_treinar(self, X, y, epochs, learning_rate):
    for epoch in range(epochs):
        # Embaralhar dados
        indices = np.random.permutation(len(X))
        
        for i in indices:
            # Uma amostra por vez
            x_sample = X[i:i+1]
            y_sample = y[i:i+1]
            
            # Forward + backward + update
            self._update_weights(x_sample, y_sample, learning_rate)
```

#### 3. Mini-batch Gradient Descent
```python
def minibatch_treinar(self, X, y, epochs, learning_rate, batch_size=32):
    for epoch in range(epochs):
        for i in range(0, len(X), batch_size):
            batch_X = X[i:i+batch_size]
            batch_y = y[i:i+batch_size]
            
            self._update_weights(batch_X, batch_y, learning_rate)
```

## üé≤ Algoritmos de Inicializa√ß√£o

### 1. Inicializa√ß√£o Xavier/Glorot
```python
def inicializacao_xavier(entrada_size, saida_size):
    limite = np.sqrt(6.0 / (entrada_size + saida_size))
    return np.random.uniform(-limite, limite, (entrada_size, saida_size))
```

**Justificativa**: Mant√©m a vari√¢ncia das ativa√ß√µes constante atrav√©s das camadas.

### 2. Inicializa√ß√£o He
```python
def inicializacao_he(entrada_size, saida_size):
    return np.random.randn(entrada_size, saida_size) * np.sqrt(2.0 / entrada_size)
```

**Justificativa**: Adequada para ReLU, considera apenas as conex√µes de entrada.

### 3. Inicializa√ß√£o Zero (‚ùå N√£o recomendada)
```python
# PROBLEMA: Todos os neur√¥nios aprendem a mesma fun√ß√£o
W = np.zeros((entrada_size, saida_size))
```

## üìä Algoritmos de Avalia√ß√£o

### C√°lculo de M√©tricas
```python
def calcular_metricas(y_true, y_pred, limiar=0.5):
    # Converter probabilidades para classes
    y_pred_bin = (y_pred >= limiar).astype(int)
    
    # Matriz de confus√£o
    tp = np.sum((y_true == 1) & (y_pred_bin == 1))
    tn = np.sum((y_true == 0) & (y_pred_bin == 0))
    fp = np.sum((y_true == 0) & (y_pred_bin == 1))
    fn = np.sum((y_true == 1) & (y_pred_bin == 0))
    
    # M√©tricas derivadas
    acuracia = (tp + tn) / (tp + tn + fp + fn)
    precisao = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precisao * recall) / (precisao + recall) if (precisao + recall) > 0 else 0
    
    return {
        'acuracia': acuracia,
        'precisao': precisao,
        'recall': recall,
        'f1_score': f1
    }
```

## üîß Algoritmos de Normaliza√ß√£o

### 1. Z-Score (Padroniza√ß√£o)
```python
def normalizacao_zscore(X):
    media = np.mean(X, axis=0)
    desvio = np.std(X, axis=0)
    X_norm = (X - media) / (desvio + 1e-8)  # Evitar divis√£o por zero
    return X_norm, {'media': media, 'desvio': desvio}
```

### 2. Min-Max Scaling
```python
def normalizacao_minmax(X):
    minimo = np.min(X, axis=0)
    maximo = np.max(X, axis=0)
    X_norm = (X - minimo) / (maximo - minimo + 1e-8)
    return X_norm, {'min': minimo, 'max': maximo}
```

### 3. Normaliza√ß√£o Robusta
```python
def normalizacao_robusta(X):
    mediana = np.median(X, axis=0)
    q75 = np.percentile(X, 75, axis=0)
    q25 = np.percentile(X, 25, axis=0)
    iqr = q75 - q25
    X_norm = (X - mediana) / (iqr + 1e-8)
    return X_norm, {'mediana': mediana, 'iqr': iqr}
```

## ‚ö° Otimiza√ß√µes Implementadas

### 1. Clipping de Gradientes (Previne Exploding)
```python
def clip_gradientes(gradientes, max_norm=5.0):
    for i, grad in enumerate(gradientes):
        norm = np.linalg.norm(grad)
        if norm > max_norm:
            gradientes[i] = grad * (max_norm / norm)
    return gradientes
```

### 2. Estabiliza√ß√£o Num√©rica na Sigmoid
```python
def sigmoid_estavel(x):
    # Evita overflow clippando valores extremos
    x_clipped = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x_clipped))
```

### 3. Detec√ß√£o de Converg√™ncia
```python
def convergiu(historico_erro, janela=50, tolerancia=1e-6):
    if len(historico_erro) < janela:
        return False
    
    erro_recente = historico_erro[-janela:]
    variacao = np.std(erro_recente)
    
    return variacao < tolerancia
```

## üìà Algoritmos de An√°lise

### 1. An√°lise de Gradientes
```python
def analisar_gradientes(self):
    normas_grad = []
    for grad in self.gradientes_recentes:
        norma = np.linalg.norm(grad)
        normas_grad.append(norma)
    
    return {
        'media': np.mean(normas_grad),
        'desvio': np.std(normas_grad),
        'max': np.max(normas_grad),
        'min': np.min(normas_grad)
    }
```

### 2. An√°lise de Ativa√ß√µes
```python
def analisar_ativacoes(self, X):
    ativacoes, _ = self.forward_propagation(X)
    
    estatisticas = []
    for i, ativacao in enumerate(ativacoes[1:]):  # Pular entrada
        stats = {
            'camada': i + 1,
            'media': np.mean(ativacao),
            'desvio': np.std(ativacao),
            'zeros': np.sum(ativacao == 0) / ativacao.size,  # Para ReLU
            'saturacao': np.sum(ativacao > 0.99) / ativacao.size  # Para sigmoid
        }
        estatisticas.append(stats)
    
    return estatisticas
```

## üöÄ Algoritmos de Acelera√ß√£o (Futuras Implementa√ß√µes)

### 1. Adam Optimizer
```python
def adam_update(self, gradientes, m, v, t, lr=0.001, beta1=0.9, beta2=0.999):
    """
    m: primeira estimativa de momento
    v: segunda estimativa de momento  
    t: passo de tempo
    """
    for i, grad in enumerate(gradientes):
        # Atualizar momentos
        m[i] = beta1 * m[i] + (1 - beta1) * grad
        v[i] = beta2 * v[i] + (1 - beta2) * (grad ** 2)
        
        # Corre√ß√£o de bias
        m_corrected = m[i] / (1 - beta1 ** t)
        v_corrected = v[i] / (1 - beta2 ** t)
        
        # Atualiza√ß√£o de par√¢metros
        self.parametros[i] -= lr * m_corrected / (np.sqrt(v_corrected) + 1e-8)
```

### 2. Learning Rate Scheduling
```python
def schedule_learning_rate(epoch, lr_inicial, metodo='step'):
    if metodo == 'step':
        return lr_inicial * (0.1 ** (epoch // 30))
    elif metodo == 'exponential':
        return lr_inicial * (0.95 ** epoch)
    elif metodo == 'cosine':
        return lr_inicial * (1 + np.cos(np.pi * epoch / max_epochs)) / 2
```

---
**Autor**: [S√°vio](https://github.com/SavioCodes)
**√öltima atualiza√ß√£o**: Janeiro 2025

# üìö Fundamentos Te√≥ricos das Redes Neurais

Este documento explica os conceitos te√≥ricos por tr√°s da implementa√ß√£o da rede neural.

## üß† Introdu√ß√£o √†s Redes Neurais

### O que s√£o Redes Neurais?

As redes neurais artificiais s√£o modelos computacionais inspirados no funcionamento do c√©rebro humano. Elas s√£o compostas por unidades de processamento simples (neur√¥nios artificiais) que trabalham em conjunto para resolver problemas complexos.

### Componentes B√°sicos

#### 1. Neur√¥nio Artificial (Perceptron)
```
Entradas: x‚ÇÅ, x‚ÇÇ, ..., x‚Çô
Pesos: w‚ÇÅ, w‚ÇÇ, ..., w‚Çô
Bias: b
Sa√≠da: y = f(‚àë(x·µ¢ √ó w·µ¢) + b)
```

#### 2. Fun√ß√£o de Ativa√ß√£o
Introduz n√£o-linearidade no modelo:
- **Sigmoid**: œÉ(x) = 1/(1 + e‚ÅªÀ£)
- **ReLU**: f(x) = max(0, x)
- **Tanh**: f(x) = (eÀ£ - e‚ÅªÀ£)/(eÀ£ + e‚ÅªÀ£)

## üîÑ Forward Propagation (Propaga√ß√£o Direta)

### Processo
1. Os dados de entrada fluem da primeira camada at√© a sa√≠da
2. Cada neur√¥nio calcula uma combina√ß√£o linear das entradas
3. Aplica uma fun√ß√£o de ativa√ß√£o
4. Passa o resultado para a pr√≥xima camada

### Matem√°tica
Para uma camada l:
```
z‚ÅΩÀ°‚Åæ = W‚ÅΩÀ°‚Åæ √ó a‚ÅΩÀ°‚Åª¬π‚Åæ + b‚ÅΩÀ°‚Åæ
a‚ÅΩÀ°‚Åæ = f(z‚ÅΩÀ°‚Åæ)
```

Onde:
- `z‚ÅΩÀ°‚Åæ`: Soma ponderada da camada l
- `W‚ÅΩÀ°‚Åæ`: Matriz de pesos da camada l
- `a‚ÅΩÀ°‚Åª¬π‚Åæ`: Ativa√ß√µes da camada anterior
- `b‚ÅΩÀ°‚Åæ`: Vetor de bias da camada l
- `f`: Fun√ß√£o de ativa√ß√£o

## ‚¨ÖÔ∏è Backpropagation (Retropropaga√ß√£o)

### Objetivo
Calcular os gradientes da fun√ß√£o de custo em rela√ß√£o aos pesos e biases, permitindo atualizar os par√¢metros para minimizar o erro.

### Algoritmo
1. **Calcular erro da sa√≠da**: Œ¥‚ÅΩ·¥∏‚Åæ = (a‚ÅΩ·¥∏‚Åæ - y) ‚äô f'(z‚ÅΩ·¥∏‚Åæ)
2. **Propagar erro para tr√°s**: Œ¥‚ÅΩÀ°‚Åæ = (W‚ÅΩÀ°‚Å∫¬π‚Åæ)·µÄ √ó Œ¥‚ÅΩÀ°‚Å∫¬π‚Åæ ‚äô f'(z‚ÅΩÀ°‚Åæ)
3. **Calcular gradientes**:
   - ‚àÇC/‚àÇW‚ÅΩÀ°‚Åæ = Œ¥‚ÅΩÀ°‚Åæ √ó (a‚ÅΩÀ°‚Åª¬π‚Åæ)·µÄ
   - ‚àÇC/‚àÇb‚ÅΩÀ°‚Åæ = Œ¥‚ÅΩÀ°‚Åæ

### Nota√ß√£o
- `‚äô`: Produto elemento por elemento (Hadamard)
- `L`: √çndice da √∫ltima camada
- `Œ¥‚ÅΩÀ°‚Åæ`: Erro da camada l
- `f'`: Derivada da fun√ß√£o de ativa√ß√£o

## üéØ Fun√ß√£o de Custo

### Erro Quadr√°tico M√©dio (MSE)
```
C = (1/2m) √ó ‚àë·µ¢(y·µ¢ - ≈∑·µ¢)¬≤
```

### Entropia Cruzada (Cross-Entropy)
```
C = -(1/m) √ó ‚àë·µ¢[y·µ¢ √ó log(≈∑·µ¢) + (1-y·µ¢) √ó log(1-≈∑·µ¢)]
```

## üîß Gradiente Descendente

### Atualiza√ß√£o dos Par√¢metros
```
W‚ÅΩÀ°‚Åæ := W‚ÅΩÀ°‚Åæ - Œ± √ó ‚àÇC/‚àÇW‚ÅΩÀ°‚Åæ
b‚ÅΩÀ°‚Åæ := b‚ÅΩÀ°‚Åæ - Œ± √ó ‚àÇC/‚àÇb‚ÅΩÀ°‚Åæ
```

Onde `Œ±` √© a taxa de aprendizado (learning rate).

### Tipos de Gradiente Descendente
1. **Batch**: Usa todo o dataset
2. **Stochastic (SGD)**: Usa uma amostra por vez
3. **Mini-batch**: Usa pequenos grupos de amostras

## ‚öñÔ∏è Inicializa√ß√£o de Pesos

### Import√¢ncia
A inicializa√ß√£o adequada dos pesos √© crucial para:
- Evitar vanishing/exploding gradients
- Garantir converg√™ncia
- Acelerar o treinamento

### M√©todos

#### 1. Inicializa√ß√£o Xavier/Glorot
```
W ~ Uniform(-‚àö(6/(n·µ¢‚Çô + n‚Çí·µ§‚Çú)), ‚àö(6/(n·µ¢‚Çô + n‚Çí·µ§‚Çú)))
```
Ideal para sigmoid e tanh.

#### 2. Inicializa√ß√£o He
```
W ~ Normal(0, ‚àö(2/n·µ¢‚Çô))
```
Ideal para ReLU.

## üé™ Fun√ß√µes de Ativa√ß√£o Detalhadas

### 1. Sigmoid
**Fun√ß√£o**: œÉ(x) = 1/(1 + e‚ÅªÀ£)
**Derivada**: œÉ'(x) = œÉ(x) √ó (1 - œÉ(x))
**Caracter√≠sticas**:
- Sa√≠da entre 0 e 1
- Interpret√°vel como probabilidade
- Problema: vanishing gradient

### 2. ReLU (Rectified Linear Unit)
**Fun√ß√£o**: f(x) = max(0, x)
**Derivada**: f'(x) = 1 se x > 0, sen√£o 0
**Caracter√≠sticas**:
- Computacionalmente eficiente
- Sem satura√ß√£o para valores positivos
- Problema: dead neurons

### 3. Tanh
**Fun√ß√£o**: tanh(x) = (eÀ£ - e‚ÅªÀ£)/(eÀ£ + e‚ÅªÀ£)
**Derivada**: tanh'(x) = 1 - tanh¬≤(x)
**Caracter√≠sticas**:
- Sa√≠da entre -1 e 1
- Zero-centered
- Similar ao sigmoid, mas melhor

## üìä M√©tricas de Avalia√ß√£o

### Classifica√ß√£o Bin√°ria

#### Matriz de Confus√£o
```
             Predito
Real    |  0  |  1  |
--------|-----|-----|
   0    | TN  | FP  |
   1    | FN  | TP  |
```

#### M√©tricas Derivadas
- **Acur√°cia**: (TP + TN) / (TP + TN + FP + FN)
- **Precis√£o**: TP / (TP + FP)
- **Recall**: TP / (TP + FN)
- **F1-Score**: 2 √ó (Precis√£o √ó Recall) / (Precis√£o + Recall)

## üö´ Problemas Comuns

### 1. Overfitting
**Sintomas**: Boa performance no treino, ruim no teste
**Solu√ß√µes**:
- Regulariza√ß√£o (L1, L2)
- Dropout
- Early stopping
- Mais dados

### 2. Underfitting
**Sintomas**: Performance ruim em treino e teste
**Solu√ß√µes**:
- Modelo mais complexo
- Mais features
- Menos regulariza√ß√£o

### 3. Vanishing Gradient
**Sintomas**: Gradientes muito pequenos nas primeiras camadas
**Solu√ß√µes**:
- ReLU em vez de sigmoid
- Inicializa√ß√£o adequada
- Batch normalization

### 4. Exploding Gradient
**Sintomas**: Gradientes muito grandes
**Solu√ß√µes**:
- Gradient clipping
- Taxa de aprendizado menor
- Inicializa√ß√£o adequada

## üìñ Refer√™ncias Te√≥ricas

1. **Rosenblatt, F. (1958)** - "The Perceptron: A Probabilistic Model"
2. **Rumelhart, D. E. et al. (1986)** - "Learning representations by back-propagating errors"
3. **Glorot, X. & Bengio, Y. (2010)** - "Understanding the difficulty of training deep feedforward neural networks"
4. **He, K. et al. (2015)** - "Delving Deep into Rectifiers"

## üéì Para Aprofundar

### Livros
- "Neural Networks and Deep Learning" - Michael Nielsen
- "Deep Learning" - Ian Goodfellow, Yoshua Bengio, Aaron Courville
- "Pattern Recognition and Machine Learning" - Christopher Bishop

### Cursos Online
- CS231n: Convolutional Neural Networks (Stanford)
- Deep Learning Specialization (Coursera)
- Fast.ai Practical Deep Learning

### Implementa√ß√µes de Refer√™ncia
- Numpy-based implementations
- Micrograd (Andrej Karpathy)
- Neural Networks from Scratch (Harrison Kinsley)

---
**Autor**: [S√°vio](https://github.com/SavioCodes)
**√öltima atualiza√ß√£o**: Janeiro 2025
